{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG-KG: Implementation Walkthrough (SIGIR '24 Aligned)\n",
                "\n",
                "This notebook provides a step-by-step walkthrough of the RAG-KG Customer Service QA System, aligned with the LinkedIn SIGIR '24 research paper: **\"Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering\"**.\n",
                "\n",
                "## Objectives\n",
                "1. Understand the Dual-Level Knowledge Graph architecture.\n",
                "2. Implement Entity-Section mapping for precise extraction.\n",
                "3. Apply $S_{T_i}$ scoring for ticket-level retrieval.\n",
                "4. Use LLM-driven subgraph extraction for context generation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup\n",
                "Ensure you have Neo4j, Qdrant, and Ollama running."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from dotenv import load_dotenv\n",
                "from app.query_processor import QueryProcessor\n",
                "from app.retrieval_system import RetrievalSystem\n",
                "from app.answer_generator import AnswerGenerator\n",
                "\n",
                "load_dotenv()\n",
                "print(\"Environment loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Query Processing (SIGIR '24 Entity-Section Mapping)\n",
                "The paper uses a `Map(Section -> Value)` for entity extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "processor = QueryProcessor()\n",
                "query = \"How to fix the csv upload error in the production dashboard?\"\n",
                "processed = processor.process(query)\n",
                "\n",
                "print(f\"Extracted Entities: {json.dumps(processed['entities'], indent=2)}\")\n",
                "print(f\"Detected Intent: {processed['intent']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Retrieval with $S_{T_i}$ Scoring\n",
                "The system calculates a ticket-level score by summing similarity contributions from specific category nodes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "retriever = RetrievalSystem()\n",
                "retriever.initialize()\n",
                "\n",
                "sources = retriever.retrieve(processed)\n",
                "for source in sources[:3]:\n",
                "    print(f\"Ticket: {source['ticket_id']} | Score: {source['score']:.2f} | Type: {source['node_type']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. LLM-driven Subgraph Extraction\n",
                "For the top-k tickets, the system generates specific Cypher queries to extract the most relevant context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This is handled internally by the retriever.retrieve() call now!\n",
                "print(f\"Retrieved {len(sources)} context nodes across relevant tickets.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Answer Generation\n",
                "Finally, the LLM constructs an answer using the extracted subgraphs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "generator = AnswerGenerator()\n",
                "answer, confidence = generator.generate(query, sources, processed)\n",
                "\n",
                "print(f\"Answer: {answer}\")\n",
                "print(f\"Confidence: {confidence:.2f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}